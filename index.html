<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title> Learning Visual Locomotion with Cross-Modal Supervision</title>
  <link href="res/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="res/academicons-1.9.2/css/academicons.min.css"/>
  <script src="res/js/fa.js"></script>

	<style>
      .header {
        width: auto;
        max-width: auto;
        padding-top: 4rem;
        padding-bottom: 2rem;
        margin-bottom: 2rem;
        background-color: rgba(250, 250, 250)
      }

      a:link,a:visited
      {
        color: #0071bc;
        text-decoration: none;
      }

      a:hover {
        color: #208799;
      }

      hr {
        border: 10;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 113, 188, 0.2), rgba(0, 0, 0, 0.1), rgba(0, 113, 188, 0.2));
      }

      .gap-30 {height:30px;}
      .gap-20 {height:20px;}
      .gap-10 {height:10px;}
      .gap-5 {height:5px;}

      .no-gutters {
      margin-right: 0;
      margin-left: 0;
    }

    .btn {
      margin-left: 0.2rem;
      margin-right: 0.2rem;
		}


	</style>
</head>

<div class="header">
	<div class="container">
		<center><h1>Learning Visual Locomotion with Cross-Modal Supervision</h1></center>
		<div class="gap-20"></div>
		<!--------------------- Author Names --------------------->
		<center>
			<a href="https://antonilo.github.io/" target="_blank" style="font-size:20px">Antonio Loquercio</a><sup
						style="font-size:14px">*</sup>,
			<a href="https://ashish-kmr.github.io/" target="_blank" style="font-size:20px">Ashish Kumar</a><sup
						style="font-size:14px">*</sup>,
			<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank" style="font-size:20px">Jitendra Malik</a><sup
						style="font-size:14px"></sup>
		</center>

		<div class="gap-10"></div>
		<center>
			UC Berkeley
		</center>

		<div class="gap-20"></div>

		<center>
			<div class="btn-group" role="group">

				<a href="pdf/manuscript.pdf">
					<button type="button" class="btn btn-dark">
						<span class="icon">
              <i class="fas fa-file-pdf"></i> Paper
            </span>
					</button>
				</a>

				<a href="https://arxiv.org/abs/2211.03785">
					<button type="button" class="btn btn-dark">
          <span><i class="ai ai-arxiv"></i> Arxiv</span>
					</button>
				</a>


				<a href="https://twitter.com/antoniloq/status/1590152618617475072?s=20&t=ui_c_kS42q-_gY09mXtEMQ">
					<button type="button" class="btn btn-dark">
						<span class="icon">
              <i class="fab fa-twitter"></i> Summary
            </span>
					</button>
				</a>

				<a href="https://github.com/antonilo/vision_locomotion">
					<button type="button" class="btn btn-dark">
						<span class="icon">
              <i class="fab fa-github"></i> Code
            </span>
					</button>
				</a>

				<a href="https://youtu.be/as5AC-3r75U">
					<button type="button" class="btn btn-dark">
						<span class="icon">
              <i class="fab fa-youtube"></i> Pitch Video
            </span>
					</button>
				</a>

			</div>
		</center>

	</div>
</div>

<div class="container">

    <center>
    <iframe width="70%" height="400" src="https://www.youtube.com/embed/d7I34YIdMdk" title="Learning to See" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </center>


    <center>
        <p style="width:90%; text-align: center; white-space: normal;">
    <b>TL;DR: Learning to walk from pixels in the real world by using proprioception as supervision. </b>
    </center>



    <!--------------------- abstract --------------------->
    <div class="gap-30"></div>
    <h2 style="text-align: center"> Abstract </h2>
    <div class="gap-10"></div>
    <center>
        <p style="width:90%; text-align: justify; white-space: normal;"> 
In this work, we show how to learn a visual walking policy that only uses a monocular RGB camera and proprioception to walk. Since simulating RGB is hard, we necessarily have to learn vision in the real world. We start with a blind walking policy trained in simulation. This policy can traverse some terrains in the real world but often struggles since it lacks knowledge of the upcoming geometry. This can be resolved with the use of vision. We train a visual module in the real world to predict the upcoming terrain with our proposed algorithm Cross-Modal Supervision (CMS). CMS uses time-shifted proprioception to supervise vision and allows the policy to continually improve with more real-world experience. 
We evaluate our vision-based walking policy over a diverse set of terrains including stairs (up to 19cm high), slippery slopes (inclination of 35 degrees), curbs and tall steps (up to 20cm), and complex discrete terrains. We achieve this performance with less than 30 minutes of real-world data.
Finally, we show that our policy can adapt to shifts in the visual field with a limited amount of real-world experience.</p>
    </center>


    <h2 style="text-align: center"> Visual Plasticity: The Prism-Adaptation Experiment </h2>

    <div class="gap-10"></div>
    <center><p style="width:90%; text-align: justify; white-space: normal;">
      We study how quickly the policy can adapt to shifts in the visual field. To do so, we change the camera orientation. This results in a large variation in the field of view, as shown in the image below. Note that after rotation, the robot cannot see the terrain in front of it.
    </p></center>

    <center>
		<img class="img-fluid" src="images/prism-expt_camera.png" style="margin-bottom: 30px" width="98%" alt=""><br>
	</center>



        <div class="gap-10"></div>
    <center><p style="width:90%; text-align: justify; white-space: normal;">
Before shifting the camera's visual field (pre-test), the policy can climb the testing staircase perfectly. However, after rotating the camera, the visual policy stumbles on stairs and drifts in the horizontal direction (exposure). After only three trials (approximately 80 seconds of data), the policy can again anticipate steps and walk without drifting (adaptation).
    </p></center>


    <div class="row no-gutters">
        <div class="col-4">
          <center> <b>Pre-Test</b></center>
            <video controls playsinline autoplay loop muted src="res/prism/pre-test.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col-4">
          <center> <b>Exposure</b></center>
            <video controls playsinline autoplay loop muted src="res/prism/exposure.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col-4">
          <center> <b>Adaptation</b></center>
            <video controls playsinline autoplay loop muted src="res/prism/adaptation.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>


        <div class="gap-10"></div>
    <center><p style="width:90%; text-align: justify; white-space: normal;">
      In the final session of the experiment (post-test), we switch back the camera to its original position. We observe that after training on only two trials, the policy can re-adapt to the original visual field.
    </p></center>

    <div class="row no-gutters">
        <div class="col-4">
          <center> <b>Post-Test: Trial I</b></center>
            <video controls playsinline autoplay loop muted src="res/prism/post-test_initial.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col-4">
          <center> <b>Post-Test: Trial II</b></center>
            <video controls playsinline autoplay loop muted src="res/prism/post-test_after_one.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col-4">
          <center> <b>Post-Test: Trial III</b></center>
            <video controls playsinline autoplay loop muted src="res/prism/post-test_adapted.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>


    	<hr>
    <h2 style="text-align: center"> Generalization Results </h2>

    <div class="gap-10"></div>
    <center><p style="width:90%; text-align: justify; white-space: normal;">
        We show that our vision-based policy can walk on previously unseen terrains.
    </p></center>

    <div class="row no-gutters">
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/gen_discrete.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/gen_construction.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>
    <div class="row no-gutters">
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/gen_upstairs.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/gen_discrete_2.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>
    <hr>


    <h2 style="text-align: center"> Generalization Results: Trip to Stanford </h2>

    <div class="gap-10"></div>
    <center><p style="width:90%; text-align: justify; white-space: normal;">
        After training our robot on a large set of terrains in the Berkeley campus, we verified generalization of the visual policy in the Stanford campus.
    </p></center>

    <div class="row no-gutters">
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/stanford/library.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/stanford/walking_around_big.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>
    <div class="row no-gutters">
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/stanford/entrance.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/stanford/burgers_and_hall.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>


    <hr>


    <hr>
    <h2 style="text-align: center"> Blind vs Visual Locomotion </h2>

    <div class="gap-10"></div>
    <center><p style="width:90%; text-align: justify; white-space: normal;">
        We show that our policy is better than the blind policy in several environments.
    </p></center>


    <div class="row no-gutters">
        <div class="col">
          <center> <b>Blind</b></center>
            <video controls playsinline autoplay loop muted src="res/blind_side.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col">
          <center> <b>Vision</b> </center>
            <video controls playsinline autoplay loop muted src="res/vision_side.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>
    <div class="row no-gutters">
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/blind_indoor.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/vision_indoor.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>
    <div class="row no-gutters">
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/long_stairs_blind.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/long_stairs_vision.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>
    <div class="row no-gutters">
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/flat_blind.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
        <div class="col">
            <video controls playsinline autoplay loop muted src="res/flat_vision.mp4" width="97%"
                   style="border-radius:10px; border:1px solid black"></video>
        </div>
    </div>


    <hr>



      	<h2 style="text-align: center"> Bibtex </h2>
	<pre style="background-color: #f4f4f4;">

  @InProceedings{loquercio2023learn,
   author={Loquercio, Antonio and Kumar, and Malik, Jitendra},
   title={{Learning Visual Locomotion with Cross-Modal Supervision}},
   booktitle={International Conference on Robotics and Automation},
   year={2023}
  }

  </pre>
	<hr>

	<b><span style="font-size:22px">Acknowledgements:</span></b><br>

This work was supported by the DARPA Machine Common Sense program and by the ONR MURI award N00014-21-1-2801. We would like to thank Sasha Sax for the helpful discussions, Noemi Aepli for support with media material, and Haozhi Qi for support with the website creation.

	<p style="text-align: right"><a href="https://github.com/HaozhiQi/haozhiqi.github.io/tree/master/hora" style="font-size:12px;">Template for this Website</a></p>



    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="res/js/bootstrap.min.js"></script>

</div>
</html>
